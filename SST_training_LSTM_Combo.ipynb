{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of SST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext as tt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pytorch_extras import RAdam, SingleCycleScheduler\n",
    "from transformers import AlbertModel, AlbertTokenizer\n",
    "from pytorch_transformers import GPT2Model, GPT2Tokenizer\n",
    "from deps.torch_train_test_loop.torch_train_test_loop import LoopComponent, TrainTestLoop\n",
    "\n",
    "from models import SSTClassifierCapsuleLSTMCombo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained transformer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "#     'gpt2-large', do_lower_case=False)\n",
    "# lang_model = GPT2Model.from_pretrained(\n",
    "#     'gpt2-large', output_hidden_states=True, output_attentions=False)\n",
    "# lang_model.cuda(device=DEVICE)\n",
    "# lang_model.eval()\n",
    "# print('Pretrained GPT-2 loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained albert loaded.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained(\n",
    "    'albert-base-v2', do_lower_case=False)\n",
    "lang_model = AlbertModel.from_pretrained(\n",
    "    'albert-base-v2', output_hidden_states=True, output_attentions=False)\n",
    "lang_model.cuda(device=DEVICE)\n",
    "lang_model.eval()\n",
    "print('Pretrained albert loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_texts_to_embs(tokenized_texts, pad_token=tokenizer.eos_token):\n",
    "    tokenized_texts = [[*tok_seq, pad_token] for tok_seq in tokenized_texts]\n",
    "    lengths = [len(tok_seq) for tok_seq in tokenized_texts]\n",
    "\n",
    "    max_length = max(lengths)\n",
    "    input_toks = [t + [pad_token] * (max_length - l) for t, l in zip(tokenized_texts, lengths)]\n",
    "\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(tok_seq) for tok_seq in input_toks]\n",
    "    input_ids = torch.tensor(input_ids).to(device=DEVICE)\n",
    "\n",
    "    mask = [[1.0] * length + [0.0] * (max_length - length) for length in lengths]\n",
    "    mask = torch.tensor(mask).to(device=DEVICE)  # [batch sz, num toks]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = lang_model(input_ids=input_ids)\n",
    "        embs = torch.stack(outputs[-1], -2)  # [batch sz, n toks, n layers, d emb]\n",
    "\n",
    "    return mask, embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_grained = False  # set to False for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTFilter():\n",
    "\n",
    "    def __init__(self, remove_neutral=False, remove_dupes=False):\n",
    "        self.remove_neutral, self.remove_dupes  = (remove_neutral, remove_dupes)\n",
    "        self.prev_seen = {}\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if self.remove_neutral and (sample.label == 'neutral'):\n",
    "            return False\n",
    "        hashable = ''.join(sample.text)\n",
    "        if self.remove_dupes and (hashable in self.prev_seen):\n",
    "            return False\n",
    "        self.prev_seen[hashable] = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready.\n",
      "Number of samples: 77,612 train phrases, 872 valid sentences, 1,821 test sentences.\n"
     ]
    }
   ],
   "source": [
    "tt.datasets.SST.download(root='.data')  # download if necessary\n",
    "_stoi = { s: i for i, s in enumerate(\n",
    "    ['very negative', 'negative', 'neutral', 'positive', 'very positive'] \\\n",
    "    if fine_grained else ['negative', 'positive']\n",
    ") }\n",
    "TEXT = tt.data.RawField(\n",
    "    preprocessing=tokenizer.tokenize,\n",
    "    postprocessing=tokenized_texts_to_embs,\n",
    "    is_target=False)\n",
    "LABEL = tt.data.RawField(\n",
    "    postprocessing=lambda samples: torch.tensor([_stoi[s] for s in samples], device=DEVICE),\n",
    "    is_target=True)\n",
    "\n",
    "trn_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/train.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=True,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=True))\n",
    "val_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/dev.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    "tst_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/test.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    "\n",
    "print('Datasets ready.')\n",
    "print('Number of samples: {:,} train phrases, {:,} valid sentences, {:,} test sentences.'\\\n",
    "      .format(len(trn_ds), len(val_ds), len(tst_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _stoi = { s: i for i, s in enumerate(\n",
    "#     ['very negative', 'negative', 'neutral', 'positive', 'very positive'] \\\n",
    "#     if fine_grained else ['neg', 'pos']\n",
    "# ) }\n",
    "\n",
    "# TEXT = tt.data.RawField(\n",
    "#     preprocessing=tokenizer.tokenize,\n",
    "#     postprocessing=tokenized_texts_to_embs,\n",
    "#     is_target=False)\n",
    "# LABEL = tt.data.RawField(\n",
    "#     postprocessing=lambda samples: torch.tensor([_stoi[s] for s in samples], device=DEVICE),\n",
    "#     is_target=True)\n",
    "\n",
    "# trn_ds, tst_ds = tt.datasets.IMDB.splits(TEXT, LABEL)\n",
    "# # trn_ds = tt.datasets.IMDB(\n",
    "# #     '.data/sst/trees/train.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=True,\n",
    "# #     filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=True))\n",
    "# # tst_ds = tt.datasets.IMDB(\n",
    "# #     '.data/sst/trees/test.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "# #     filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    " \n",
    "# trn_ds, val_ds = trn_ds.split()\n",
    "\n",
    "# print('Datasets ready.')\n",
    "# print('Number of samples: {:,} train phrases, {:,} valid sentences, {:,} test sentences.'\\\n",
    "#       .format(len(trn_ds), len(val_ds), len(tst_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x7f426ec662b0>\n"
     ]
    }
   ],
   "source": [
    "print(val_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function = nn.MultiMarginLoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "class LoopMain(LoopComponent):\n",
    "\n",
    "    def __init__(self, n_classes, device, pct_warmup=0.1, mixup=(0.2, 0.2)):\n",
    "        self.n_classes, self.device, self.pct_warmup = (n_classes, device, pct_warmup)\n",
    "        self.mixup_dist = torch.distributions.Beta(torch.tensor(mixup[0]), torch.tensor(mixup[1]))\n",
    "        self.onehot = torch.eye(self.n_classes, device=self.device)\n",
    "        self.saved_data = []\n",
    "\n",
    "    def on_train_begin(self, loop):\n",
    "        n_iters = len(loop.train_data) * loop.n_epochs\n",
    "        loop.optimizer = RAdam(loop.model.parameters(), lr=5e-4)\n",
    "        loop.scheduler = SingleCycleScheduler(\n",
    "            loop.optimizer, loop.n_optim_steps, frac=self.pct_warmup, min_lr=1e-5)\n",
    "        \n",
    "    def on_grads_reset(self, loop):\n",
    "        loop.model.zero_grad()\n",
    "\n",
    "    def on_forward_pass(self, loop):\n",
    "        model, batch = (loop.model, loop.batch)\n",
    "        mask, embs = batch.text\n",
    "        target_probs = self.onehot[batch.label]\n",
    "\n",
    "        if loop.is_training:\n",
    "            r = self.mixup_dist.sample([len(mask)]).to(device=mask.device)\n",
    "            idx = torch.randperm(len(mask))\n",
    "            mask = mask.lerp(mask[idx], r[:, None])\n",
    "            embs = embs.lerp(embs[idx], r[:, None, None, None])\n",
    "            target_probs = target_probs.lerp(target_probs[idx], r[:, None])\n",
    "\n",
    "        pred_scores, _, _ = model(mask, embs)\n",
    "        _, pred_ids = pred_scores.max(-1)\n",
    "        accuracy = (pred_ids == batch.label).float().mean()\n",
    "\n",
    "        loop.pred_scores, loop.target_probs, loop.accuracy = (pred_scores, target_probs, accuracy)\n",
    "\n",
    "    def on_loss_compute(self, loop):\n",
    "        _, targets = loop.target_probs.max(dim=1)\n",
    "        losses = loss_function(loop.pred_scores, targets) # CE \n",
    "        loop.loss = losses.sum(dim=-1).mean()  # sum of classes, mean of batch\n",
    "\n",
    "    def on_backward_pass(self, loop):\n",
    "        loop.loss.backward()\n",
    "\n",
    "    def on_optim_step(self, loop):\n",
    "        loop.optimizer.step()\n",
    "        loop.scheduler.step()\n",
    "\n",
    "    def on_batch_end(self, loop):\n",
    "        self.saved_data.append({\n",
    "            'n_samples': len(loop.batch),\n",
    "            'epoch_desc': loop.epoch_desc,\n",
    "            'epoch_num': loop.epoch_num,\n",
    "            'epoch_frac': loop.epoch_num + loop.batch_num / loop.n_batches,\n",
    "            'batch_num' : loop.batch_num,\n",
    "            'accuracy': loop.accuracy.item(),\n",
    "            'loss': loop.loss.item(),\n",
    "            'lr': loop.optimizer.param_groups[0]['lr'],\n",
    "            'momentum': loop.optimizer.param_groups[0]['betas'][0],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopProgressBar(LoopComponent):\n",
    "\n",
    "    def __init__(self, item_names=['loss', 'accuracy']):\n",
    "        self.item_names = item_names\n",
    "\n",
    "    def on_epoch_begin(self, loop):\n",
    "        self.total, self.count = ({ name: 0.0 for name in self.item_names }, 0)\n",
    "        self.pbar = tqdm(total=loop.n_batches, desc=f\"{loop.epoch_desc} epoch {loop.epoch_num}\")\n",
    "\n",
    "    def on_batch_end(self, loop):\n",
    "        n = len(loop.batch)\n",
    "        self.count += n\n",
    "        for name in self.item_names:\n",
    "            self.total[name] += getattr(loop, name).item() * n\n",
    "        self.pbar.update(1)\n",
    "        if (not loop.is_training):\n",
    "            means = { f'mean_{name}': self.total[name] / self.count for name in self.item_names }\n",
    "            self.pbar.set_postfix(means)\n",
    "\n",
    "    def on_epoch_end(self, loop):\n",
    "        self.pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 379,394\n"
     ]
    }
   ],
   "source": [
    "# Seed RNG for replicability. Run at least a few times without seeding to measure performance.\n",
    "torch.manual_seed(4)\n",
    "\n",
    "# Make iterators for each split.\n",
    "trn_itr, val_itr, tst_itr = tt.data.Iterator.splits(\n",
    "    (trn_ds, val_ds, tst_ds),\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    device=DEVICE)\n",
    "\n",
    "# Initialize model.\n",
    "n_classes = len(_stoi)\n",
    "\n",
    "model = SSTClassifierCapsuleLSTMCombo(\n",
    "    d_depth=lang_model.config.num_hidden_layers + 1,\n",
    "    d_emb=lang_model.config.hidden_size,\n",
    "    d_inp=64,\n",
    "    d_cap=2,\n",
    "    n_parts=64,\n",
    "    n_classes=n_classes,\n",
    "    n_iters=3\n",
    ")\n",
    "\n",
    "model = model.cuda(device=DEVICE)\n",
    "print('Total number of parameters: {:,}'.format(sum(np.prod(p.shape) for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Train model\n",
    "# loop = TrainTestLoop(model, [LoopMain(n_classes, DEVICE), LoopProgressBar()], trn_itr, val_itr)\n",
    "# loop.train(n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 0: 100%|██████████| 1213/1213 [04:16<00:00,  4.74it/s]\n",
      "valid epoch 0: 100%|██████████| 14/14 [00:01<00:00,  7.85it/s, mean_loss=0.42, mean_accuracy=0.811] \n",
      "train epoch 1: 100%|██████████| 1213/1213 [04:17<00:00,  5.21it/s]\n",
      "valid epoch 1: 100%|██████████| 14/14 [00:01<00:00,  7.93it/s, mean_loss=0.382, mean_accuracy=0.825]\n",
      "test epoch 2: 100%|██████████| 29/29 [00:02<00:00,  7.42it/s, mean_loss=0.356, mean_accuracy=0.851]\n",
      "train epoch 0: 100%|██████████| 1213/1213 [04:17<00:00,  4.69it/s]\n",
      "valid epoch 0: 100%|██████████| 14/14 [00:01<00:00,  7.90it/s, mean_loss=0.363, mean_accuracy=0.835]\n",
      "train epoch 1: 100%|██████████| 1213/1213 [04:17<00:00,  5.17it/s]\n",
      "valid epoch 1: 100%|██████████| 14/14 [00:01<00:00,  7.87it/s, mean_loss=0.367, mean_accuracy=0.837]\n",
      "test epoch 2: 100%|██████████| 29/29 [00:02<00:00,  7.47it/s, mean_loss=0.338, mean_accuracy=0.855]\n",
      "train epoch 0: 100%|██████████| 1213/1213 [04:18<00:00,  5.84it/s]\n",
      "valid epoch 0: 100%|██████████| 14/14 [00:01<00:00,  7.78it/s, mean_loss=0.357, mean_accuracy=0.849]\n",
      "train epoch 1: 100%|██████████| 1213/1213 [04:17<00:00,  4.60it/s]\n",
      "valid epoch 1: 100%|██████████| 14/14 [00:01<00:00,  7.85it/s, mean_loss=0.356, mean_accuracy=0.852]\n",
      "test epoch 2: 100%|██████████| 29/29 [00:02<00:00,  7.48it/s, mean_loss=0.331, mean_accuracy=0.858]\n",
      "train epoch 0: 100%|██████████| 1213/1213 [04:18<00:00,  4.75it/s]\n",
      "valid epoch 0: 100%|██████████| 14/14 [00:01<00:00,  7.88it/s, mean_loss=0.367, mean_accuracy=0.853]\n",
      "train epoch 1: 100%|██████████| 1213/1213 [04:17<00:00,  4.77it/s]\n",
      "valid epoch 1: 100%|██████████| 14/14 [00:01<00:00,  7.84it/s, mean_loss=0.361, mean_accuracy=0.846]\n",
      "test epoch 2: 100%|██████████| 29/29 [00:02<00:00,  7.44it/s, mean_loss=0.332, mean_accuracy=0.86] \n",
      "train epoch 0: 100%|██████████| 1213/1213 [04:14<00:00,  5.91it/s]\n",
      "valid epoch 0: 100%|██████████| 14/14 [00:01<00:00,  7.70it/s, mean_loss=0.36, mean_accuracy=0.847] \n",
      "train epoch 1: 100%|██████████| 1213/1213 [04:14<00:00,  5.19it/s]\n",
      "valid epoch 1: 100%|██████████| 14/14 [00:01<00:00,  7.86it/s, mean_loss=0.362, mean_accuracy=0.859]\n",
      "test epoch 2: 100%|██████████| 29/29 [00:02<00:00,  7.44it/s, mean_loss=0.338, mean_accuracy=0.863]\n",
      "train epoch 0: 100%|██████████| 1213/1213 [04:13<00:00,  4.90it/s]\n",
      "valid epoch 0: 100%|██████████| 14/14 [00:01<00:00,  7.87it/s, mean_loss=0.358, mean_accuracy=0.86] \n",
      "train epoch 1: 100%|██████████| 1213/1213 [04:14<00:00,  5.68it/s]\n",
      "valid epoch 1: 100%|██████████| 14/14 [00:01<00:00,  7.76it/s, mean_loss=0.349, mean_accuracy=0.862]\n",
      "test epoch 2: 100%|██████████| 29/29 [00:02<00:00,  7.47it/s, mean_loss=0.336, mean_accuracy=0.864]\n",
      "train epoch 0: 100%|██████████| 1213/1213 [04:14<00:00,  5.24it/s]\n",
      "valid epoch 0: 100%|██████████| 14/14 [00:01<00:00,  7.80it/s, mean_loss=0.345, mean_accuracy=0.869]\n",
      "train epoch 1: 100%|██████████| 1213/1213 [04:15<00:00,  5.32it/s]\n",
      "valid epoch 1: 100%|██████████| 14/14 [00:01<00:00,  7.83it/s, mean_loss=0.356, mean_accuracy=0.867]\n",
      "test epoch 2: 100%|██████████| 29/29 [00:02<00:00,  7.50it/s, mean_loss=0.349, mean_accuracy=0.862]\n"
     ]
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    loop = TrainTestLoop(model, [LoopMain(n_classes, DEVICE), LoopProgressBar()], trn_itr, val_itr)\n",
    "    loop.train(n_epochs=2)\n",
    "    loop.test(tst_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test epoch 2: 100%|██████████| 29/29 [00:02<00:00,  7.48it/s, mean_loss=0.349, mean_accuracy=0.862]\n"
     ]
    }
   ],
   "source": [
    "loop.test(tst_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
