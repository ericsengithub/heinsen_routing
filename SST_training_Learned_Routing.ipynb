{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of SST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:49:55.086824Z",
     "start_time": "2020-06-09T03:49:51.493507Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext as tt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pytorch_extras import RAdam, SingleCycleScheduler\n",
    "from transformers import AlbertModel, AlbertTokenizer\n",
    "from pytorch_transformers import GPT2Model, GPT2Tokenizer\n",
    "from deps.torch_train_test_loop.torch_train_test_loop import LoopComponent, TrainTestLoop\n",
    "\n",
    "from models import SSTClassifierLearnedRouting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:49:55.090759Z",
     "start_time": "2020-06-09T03:49:55.087768Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained transformer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:49:55.096744Z",
     "start_time": "2020-06-09T03:49:55.092754Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "#     'gpt2-large', do_lower_case=False)\n",
    "# lang_model = GPT2Model.from_pretrained(\n",
    "#     'gpt2-large', output_hidden_states=True, output_attentions=False)\n",
    "# lang_model.cuda(device=DEVICE)\n",
    "# lang_model.eval()\n",
    "# print('Pretrained GPT-2 loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:49:58.081468Z",
     "start_time": "2020-06-09T03:49:55.098739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained albert loaded.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained(\n",
    "    'albert-base-v2', do_lower_case=False)\n",
    "lang_model = AlbertModel.from_pretrained(\n",
    "    'albert-base-v2', output_hidden_states=True, output_attentions=False)\n",
    "lang_model.cuda(device=DEVICE)\n",
    "lang_model.eval()\n",
    "print('Pretrained albert loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:49:58.088421Z",
     "start_time": "2020-06-09T03:49:58.082436Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenized_texts_to_embs(tokenized_texts, pad_token=tokenizer.eos_token):\n",
    "    tokenized_texts = [[*tok_seq, pad_token] for tok_seq in tokenized_texts]\n",
    "    lengths = [len(tok_seq) for tok_seq in tokenized_texts]\n",
    "\n",
    "    max_length = max(lengths)\n",
    "    input_toks = [t + [pad_token] * (max_length - l) for t, l in zip(tokenized_texts, lengths)]\n",
    "\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(tok_seq) for tok_seq in input_toks]\n",
    "    input_ids = torch.tensor(input_ids).to(device=DEVICE)\n",
    "\n",
    "    mask = [[1.0] * length + [0.0] * (max_length - length) for length in lengths]\n",
    "    mask = torch.tensor(mask).to(device=DEVICE)  # [batch sz, num toks]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = lang_model(input_ids=input_ids)\n",
    "        embs = torch.stack(outputs[-1], -2)  # [batch sz, n toks, n layers, d emb]\n",
    "\n",
    "    return mask, embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:49:58.092410Z",
     "start_time": "2020-06-09T03:49:58.089418Z"
    }
   },
   "outputs": [],
   "source": [
    "fine_grained = True  # set to False for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:49:58.099392Z",
     "start_time": "2020-06-09T03:49:58.093408Z"
    }
   },
   "outputs": [],
   "source": [
    "class SSTFilter():\n",
    "\n",
    "    def __init__(self, remove_neutral=False, remove_dupes=False):\n",
    "        self.remove_neutral, self.remove_dupes  = (remove_neutral, remove_dupes)\n",
    "        self.prev_seen = {}\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if self.remove_neutral and (sample.label == 'neutral'):\n",
    "            return False\n",
    "        hashable = ''.join(sample.text)\n",
    "        if self.remove_dupes and (hashable in self.prev_seen):\n",
    "            return False\n",
    "        self.prev_seen[hashable] = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:50:12.239167Z",
     "start_time": "2020-06-09T03:49:58.101386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready.\n",
      "Number of samples: 159,243 train phrases, 1,101 valid sentences, 2,210 test sentences.\n"
     ]
    }
   ],
   "source": [
    "tt.datasets.SST.download(root='.data')  # download if necessary\n",
    "_stoi = { s: i for i, s in enumerate(\n",
    "    ['very negative', 'negative', 'neutral', 'positive', 'very positive'] \\\n",
    "    if fine_grained else ['negative', 'positive']\n",
    ") }\n",
    "TEXT = tt.data.RawField(\n",
    "    preprocessing=tokenizer.tokenize,\n",
    "    postprocessing=tokenized_texts_to_embs,\n",
    "    is_target=False)\n",
    "LABEL = tt.data.RawField(\n",
    "    postprocessing=lambda samples: torch.tensor([_stoi[s] for s in samples], device=DEVICE),\n",
    "    is_target=True)\n",
    "\n",
    "trn_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/train.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=True,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=True))\n",
    "val_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/dev.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    "tst_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/test.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    "\n",
    "print('Datasets ready.')\n",
    "print('Number of samples: {:,} train phrases, {:,} valid sentences, {:,} test sentences.'\\\n",
    "      .format(len(trn_ds), len(val_ds), len(tst_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:50:12.244179Z",
     "start_time": "2020-06-09T03:50:12.241160Z"
    }
   },
   "outputs": [],
   "source": [
    "# _stoi = { s: i for i, s in enumerate(\n",
    "#     ['very negative', 'negative', 'neutral', 'positive', 'very positive'] \\\n",
    "#     if fine_grained else ['neg', 'pos']\n",
    "# ) }\n",
    "\n",
    "# TEXT = tt.data.RawField(\n",
    "#     preprocessing=tokenizer.tokenize,\n",
    "#     postprocessing=tokenized_texts_to_embs,\n",
    "#     is_target=False)\n",
    "# LABEL = tt.data.RawField(\n",
    "#     postprocessing=lambda samples: torch.tensor([_stoi[s] for s in samples], device=DEVICE),\n",
    "#     is_target=True)\n",
    "\n",
    "# trn_ds, tst_ds = tt.datasets.IMDB.splits(TEXT, LABEL)\n",
    "# # trn_ds = tt.datasets.IMDB(\n",
    "# #     '.data/sst/trees/train.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=True,\n",
    "# #     filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=True))\n",
    "# # tst_ds = tt.datasets.IMDB(\n",
    "# #     '.data/sst/trees/test.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "# #     filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    " \n",
    "# trn_ds, val_ds = trn_ds.split()\n",
    "\n",
    "# print('Datasets ready.')\n",
    "# print('Number of samples: {:,} train phrases, {:,} valid sentences, {:,} test sentences.'\\\n",
    "#       .format(len(trn_ds), len(val_ds), len(tst_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:50:12.253130Z",
     "start_time": "2020-06-09T03:50:12.245150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x7fa63d2facc0>\n"
     ]
    }
   ],
   "source": [
    "print(val_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:50:12.274074Z",
     "start_time": "2020-06-09T03:50:12.255124Z"
    }
   },
   "outputs": [],
   "source": [
    "#loss_function = nn.MultiMarginLoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "class LoopMain(LoopComponent):\n",
    "\n",
    "    def __init__(self, n_classes, device, pct_warmup=0.1, mixup=(0.2, 0.2)):\n",
    "        self.n_classes, self.device, self.pct_warmup = (n_classes, device, pct_warmup)\n",
    "        self.mixup_dist = torch.distributions.Beta(torch.tensor(mixup[0]), torch.tensor(mixup[1]))\n",
    "        self.onehot = torch.eye(self.n_classes, device=self.device)\n",
    "        self.saved_data = []\n",
    "\n",
    "    def on_train_begin(self, loop):\n",
    "        n_iters = len(loop.train_data) * loop.n_epochs\n",
    "        loop.optimizer = RAdam(loop.model.parameters(), lr=5e-4)\n",
    "        loop.scheduler = SingleCycleScheduler(\n",
    "            loop.optimizer, loop.n_optim_steps, frac=self.pct_warmup, min_lr=1e-5)\n",
    "        \n",
    "    def on_grads_reset(self, loop):\n",
    "        loop.model.zero_grad()\n",
    "\n",
    "    def on_forward_pass(self, loop):\n",
    "        model, batch = (loop.model, loop.batch)\n",
    "        mask, embs = batch.text\n",
    "        target_probs = self.onehot[batch.label]\n",
    "\n",
    "        if loop.is_training:\n",
    "            r = self.mixup_dist.sample([len(mask)]).to(device=mask.device)\n",
    "            idx = torch.randperm(len(mask))\n",
    "            mask = mask.lerp(mask[idx], r[:, None])\n",
    "            embs = embs.lerp(embs[idx], r[:, None, None, None])\n",
    "            target_probs = target_probs.lerp(target_probs[idx], r[:, None])\n",
    "\n",
    "        pred_scores, _, _ = model(mask, embs)\n",
    "        _, pred_ids = pred_scores.max(-1)\n",
    "        accuracy = (pred_ids == batch.label).float().mean()\n",
    "\n",
    "        loop.pred_scores, loop.target_probs, loop.accuracy = (pred_scores, target_probs, accuracy)\n",
    "\n",
    "    def on_loss_compute(self, loop):\n",
    "        # ls = -loop.target_probs * F.log_softmax(loop.pred_scores, dim=-1)  # CE\n",
    "        # print(ls.shape)\n",
    "        _, targets = loop.target_probs.max(dim=1)\n",
    "        losses = loss_function(loop.pred_scores, targets) # CE\n",
    "        loop.loss = losses.sum(dim=-1).mean()  # sum of classes, mean of batch\n",
    "\n",
    "    def on_backward_pass(self, loop):\n",
    "        loop.loss.backward()\n",
    "\n",
    "    def on_optim_step(self, loop):\n",
    "        loop.optimizer.step()\n",
    "        loop.scheduler.step()\n",
    "\n",
    "    def on_batch_end(self, loop):\n",
    "        self.saved_data.append({\n",
    "            'n_samples': len(loop.batch),\n",
    "            'epoch_desc': loop.epoch_desc,\n",
    "            'epoch_num': loop.epoch_num,\n",
    "            'epoch_frac': loop.epoch_num + loop.batch_num / loop.n_batches,\n",
    "            'batch_num' : loop.batch_num,\n",
    "            'accuracy': loop.accuracy.item(),\n",
    "            'loss': loop.loss.item(),\n",
    "            'lr': loop.optimizer.param_groups[0]['lr'],\n",
    "            'momentum': loop.optimizer.param_groups[0]['betas'][0],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:50:12.285045Z",
     "start_time": "2020-06-09T03:50:12.276067Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoopProgressBar(LoopComponent):\n",
    "\n",
    "    def __init__(self, item_names=['loss', 'accuracy']):\n",
    "        self.item_names = item_names\n",
    "\n",
    "    def on_epoch_begin(self, loop):\n",
    "        self.total, self.count = ({ name: 0.0 for name in self.item_names }, 0)\n",
    "        self.pbar = tqdm(total=loop.n_batches, desc=f\"{loop.epoch_desc} epoch {loop.epoch_num}\")\n",
    "\n",
    "    def on_batch_end(self, loop):\n",
    "        n = len(loop.batch)\n",
    "        self.count += n\n",
    "        for name in self.item_names:\n",
    "            self.total[name] += getattr(loop, name).item() * n\n",
    "        self.pbar.update(1)\n",
    "        if (not loop.is_training):\n",
    "            means = { f'mean_{name}': self.total[name] / self.count for name in self.item_names }\n",
    "            self.pbar.set_postfix(means)\n",
    "\n",
    "    def on_epoch_end(self, loop):\n",
    "        self.pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T03:50:12.301038Z",
     "start_time": "2020-06-09T03:50:12.288036Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 82,837\n"
     ]
    }
   ],
   "source": [
    "# Seed RNG for replicability. Run at least a few times without seeding to measure performance.\n",
    "# torch.manual_seed(<type an int here>)\n",
    "\n",
    "# Make iterators for each split.\n",
    "trn_itr, val_itr, tst_itr = tt.data.Iterator.splits(\n",
    "    (trn_ds, val_ds, tst_ds),\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    device=DEVICE)\n",
    "\n",
    "# Initialize model.\n",
    "n_classes = len(_stoi)\n",
    "\n",
    "model = SSTClassifierLearnedRouting(\n",
    "    d_depth=lang_model.config.num_hidden_layers + 1,\n",
    "    d_emb=lang_model.config.hidden_size,\n",
    "    d_inp=64,\n",
    "    d_cap=2,\n",
    "    n_parts=64,\n",
    "    n_classes=n_classes,\n",
    ")\n",
    "\n",
    "model = model.cuda(device=DEVICE)\n",
    "print('Total number of parameters: {:,}'.format(sum(np.prod(p.shape) for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T04:08:47.596112Z",
     "start_time": "2020-06-09T03:50:12.303994Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 0:   0%|          | 0/2489 [00:00<?, ?it/s]ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-14-32738c185ff8>\", line 3, in <module>\n",
      "    loop.train(n_epochs=5)\n",
      "  File \"/datasets/home/home-03/75/975/esen/heinsen_routing/deps/torch_train_test_loop/torch_train_test_loop.py\", line 104, in train\n",
      "    self._run_epoch(self.train_data, TRAIN_DESC)\n",
      "  File \"/datasets/home/home-03/75/975/esen/heinsen_routing/deps/torch_train_test_loop/torch_train_test_loop.py\", line 91, in _run_epoch\n",
      "    self._components_do('on_batch_begin', 'on_forward_pass', 'on_loss_compute')\n",
      "  File \"/datasets/home/home-03/75/975/esen/heinsen_routing/deps/torch_train_test_loop/torch_train_test_loop.py\", line 80, in _components_do\n",
      "    callback(self)\n",
      "  File \"<ipython-input-11-64387507f3ea>\", line 33, in on_forward_pass\n",
      "    pred_scores, _, _ = model(mask, embs)\n",
      "  File \"/datasets/home/75/975/esen/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/datasets/home/home-03/75/975/esen/heinsen_routing/models.py\", line 317, in forward\n",
      "    a, mu, sig2 = routing(a, mu)\n",
      "  File \"/datasets/home/75/975/esen/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/datasets/home/home-03/75/975/esen/heinsen_routing/heinsen_routing.py\", line 536, in forward\n",
      "    mu_out = mu_out * self.f(self.mu_scaler(mu_out.reshape(batch_size, -1))).reshape(mu_shape)\n",
      "RuntimeError: shape '[64, 64, 1, 2]' is invalid for input of size 4096\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'RuntimeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/conda/lib/python3.7/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/datasets/home/75/975/esen/.local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/datasets/home/75/975/esen/.local/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/opt/conda/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/datasets/home/75/975/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/__init__.py\", line 43, in <module>\n",
      "    from tensorflow.contrib import cudnn_rnn\n",
      "  File \"/datasets/home/75/975/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/__init__.py\", line 38, in <module>\n",
      "    from tensorflow.contrib.cudnn_rnn.python.layers import *\n",
      "  File \"/datasets/home/75/975/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/__init__.py\", line 38, in <module>\n",
      "    from tensorflow.contrib.cudnn_rnn.python.layers import *\n",
      "  File \"/datasets/home/75/975/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/__init__.py\", line 23, in <module>\n",
      "    from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import *\n",
      "  File \"/datasets/home/75/975/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\", line 20, in <module>\n",
      "    from tensorflow.contrib.cudnn_rnn.python.ops import cudnn_rnn_ops\n",
      "  File \"/datasets/home/75/975/esen/.local/lib/python3.7/site-packages/tensorflow_core/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py\", line 21, in <module>\n",
      "    from tensorflow.contrib.checkpoint.python import split_dependency\n",
      "ImportError: cannot import name 'split_dependency' from 'tensorflow.contrib.checkpoint.python' (unknown location)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[64, 64, 1, 2]' is invalid for input of size 4096",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "loop = TrainTestLoop(model, [LoopMain(n_classes, DEVICE), LoopProgressBar()], trn_itr, val_itr)\n",
    "loop.train(n_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-09T03:39:23.769Z"
    }
   },
   "outputs": [],
   "source": [
    "loop.test(tst_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
