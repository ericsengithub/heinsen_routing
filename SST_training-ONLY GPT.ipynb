{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of SST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext as tt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pytorch_extras import RAdam, SingleCycleScheduler\n",
    "from transformers import AlbertModel, AlbertTokenizer\n",
    "from pytorch_transformers import GPT2Model, GPT2Tokenizer\n",
    "from deps.torch_train_test_loop.torch_train_test_loop import LoopComponent, TrainTestLoop\n",
    "\n",
    "from models import SSTClassifier, GPT2Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained transformer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1042301/1042301 [00:00<00:00, 2040200.45B/s]\n",
      "100%|██████████| 456318/456318 [00:00<00:00, 1145694.44B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained GPT-2 loaded.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "    'gpt2-large', do_lower_case=False)\n",
    "# lang_model = GPT2Model.from_pretrained(\n",
    "#     'gpt2-large', output_hidden_states=True, output_attentions=False)\n",
    "# lang_model.cuda(device=DEVICE)\n",
    "# lang_model.eval()\n",
    "print('Pretrained GPT-2 loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AlbertTokenizer.from_pretrained(\n",
    "#     'albert-large-v2', do_lower_case=False)\n",
    "# lang_model = AlbertModel.from_pretrained(\n",
    "#     'albert-large-v2', output_hidden_states=True, output_attentions=False)\n",
    "# lang_model.cuda(device=DEVICE)\n",
    "# lang_model.eval()\n",
    "# print('Pretrained albert loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_texts_to_embs(tokenized_texts, pad_token=tokenizer.eos_token):\n",
    "    tokenized_texts = [[*tok_seq, pad_token] for tok_seq in tokenized_texts]\n",
    "    lengths = [len(tok_seq) for tok_seq in tokenized_texts]\n",
    "\n",
    "    max_length = max(lengths)\n",
    "    input_toks = [t + [pad_token] * (max_length - l) for t, l in zip(tokenized_texts, lengths)]\n",
    "\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(tok_seq) for tok_seq in input_toks]\n",
    "    input_ids = torch.tensor(input_ids).to(device=DEVICE)\n",
    "\n",
    "    mask = [[1.0] * length + [0.0] * (max_length - length) for length in lengths]\n",
    "    mask = torch.tensor(mask).to(device=DEVICE)  # [batch sz, num toks]\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = lang_model(input_ids=input_ids)\n",
    "#         embs = torch.stack(outputs[-1], -2)  # [batch sz, n toks, n layers, d emb]\n",
    "\n",
    "    return mask, input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_grained = True  # set to False for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTFilter():\n",
    "\n",
    "    def __init__(self, remove_neutral=False, remove_dupes=False):\n",
    "        self.remove_neutral, self.remove_dupes  = (remove_neutral, remove_dupes)\n",
    "        self.prev_seen = {}\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if self.remove_neutral and (sample.label == 'neutral'):\n",
    "            return False\n",
    "        hashable = ''.join(sample.text)\n",
    "        if self.remove_dupes and (hashable in self.prev_seen):\n",
    "            return False\n",
    "        self.prev_seen[hashable] = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready.\n",
      "Number of samples: 159,274 train phrases, 1,101 valid sentences, 2,210 test sentences.\n"
     ]
    }
   ],
   "source": [
    "# tt.datasets.SST.download(root='.data')  # download if necessary\n",
    "_stoi = { s: i for i, s in enumerate(\n",
    "    ['very negative', 'negative', 'neutral', 'positive', 'very positive'] \\\n",
    "    if fine_grained else ['negative', 'positive']\n",
    ") }\n",
    "TEXT = tt.data.RawField(\n",
    "    preprocessing=tokenizer.tokenize,\n",
    "    postprocessing=tokenized_texts_to_embs,\n",
    "    is_target=False)\n",
    "LABEL = tt.data.RawField(\n",
    "    postprocessing=lambda samples: torch.tensor([_stoi[s] for s in samples], device=DEVICE),\n",
    "    is_target=True)\n",
    "\n",
    "trn_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/train.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=True,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=True))\n",
    "val_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/dev.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    "tst_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/test.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    "\n",
    "print('Datasets ready.')\n",
    "print('Number of samples: {:,} train phrases, {:,} valid sentences, {:,} test sentences.'\\\n",
    "      .format(len(trn_ds), len(val_ds), len(tst_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _stoi = { s: i for i, s in enumerate(\n",
    "#     ['very negative', 'negative', 'neutral', 'positive', 'very positive'] \\\n",
    "#     if fine_grained else ['neg', 'pos']\n",
    "# ) }\n",
    "\n",
    "# TEXT = tt.data.RawField(\n",
    "#     preprocessing=tokenizer.tokenize,\n",
    "#     postprocessing=tokenized_texts_to_embs,\n",
    "#     is_target=False)\n",
    "# LABEL = tt.data.RawField(\n",
    "#     postprocessing=lambda samples: torch.tensor([_stoi[s] for s in samples], device=DEVICE),\n",
    "#     is_target=True)\n",
    "\n",
    "# trn_ds, tst_ds = tt.datasets.IMDB.splits(TEXT, LABEL)\n",
    "# # trn_ds = tt.datasets.IMDB(\n",
    "# #     '.data/sst/trees/train.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=True,\n",
    "# #     filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=True))\n",
    "# # tst_ds = tt.datasets.IMDB(\n",
    "# #     '.data/sst/trees/test.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "# #     filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    " \n",
    "# trn_ds, val_ds = trn_ds.split()\n",
    "\n",
    "# print('Datasets ready.')\n",
    "# print('Number of samples: {:,} train phrases, {:,} valid sentences, {:,} test sentences.'\\\n",
    "#       .format(len(trn_ds), len(val_ds), len(tst_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x7f713103a518>\n"
     ]
    }
   ],
   "source": [
    "print(val_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopMain(LoopComponent):\n",
    "\n",
    "    def __init__(self, n_classes, device, pct_warmup=0.1, mixup=(0.2, 0.2)):\n",
    "        self.n_classes, self.device, self.pct_warmup = (n_classes, device, pct_warmup)\n",
    "        self.mixup_dist = torch.distributions.Beta(torch.tensor(mixup[0]), torch.tensor(mixup[1]))\n",
    "        self.onehot = torch.eye(self.n_classes, device=self.device)\n",
    "        self.saved_data = []\n",
    "\n",
    "    def on_train_begin(self, loop):\n",
    "        n_iters = len(loop.train_data) * loop.n_epochs\n",
    "        loop.optimizer = RAdam(loop.model.parameters(), lr=5e-4)\n",
    "        loop.scheduler = SingleCycleScheduler(\n",
    "            loop.optimizer, loop.n_optim_steps, frac=self.pct_warmup, min_lr=1e-5)\n",
    "        \n",
    "    def on_grads_reset(self, loop):\n",
    "        loop.model.zero_grad()\n",
    "\n",
    "    def on_forward_pass(self, loop):\n",
    "        model, batch = (loop.model, loop.batch)\n",
    "        mask, embs = batch.text\n",
    "        target_probs = self.onehot[batch.label]\n",
    "\n",
    "#         if loop.is_training:\n",
    "#             r = self.mixup_dist.sample([len(mask)]).to(device=mask.device)\n",
    "#             idx = torch.randperm(len(mask))\n",
    "#             mask = mask.lerp(mask[idx], r[:, None])\n",
    "#             embs = embs.lerp(embs[idx], r[:, None, None, None])\n",
    "#             target_probs = target_probs.lerp(target_probs[idx], r[:, None])\n",
    "\n",
    "        pred_scores = model(mask, embs)\n",
    "        _, pred_ids = pred_scores.max(-1)\n",
    "        accuracy = (pred_ids == batch.label).float().mean()\n",
    "\n",
    "        loop.pred_scores, loop.target_probs, loop.accuracy = (pred_scores, target_probs, accuracy)\n",
    "\n",
    "    def on_loss_compute(self, loop):\n",
    "        losses = -loop.target_probs * F.log_softmax(loop.pred_scores, dim=-1)  # CE\n",
    "        loop.loss = losses.sum(dim=-1).mean()  # sum of classes, mean of batch\n",
    "\n",
    "    def on_backward_pass(self, loop):\n",
    "        loop.loss.backward()\n",
    "\n",
    "    def on_optim_step(self, loop):\n",
    "        loop.optimizer.step()\n",
    "        loop.scheduler.step()\n",
    "\n",
    "    def on_batch_end(self, loop):\n",
    "        self.saved_data.append({\n",
    "            'n_samples': len(loop.batch),\n",
    "            'epoch_desc': loop.epoch_desc,\n",
    "            'epoch_num': loop.epoch_num,\n",
    "            'epoch_frac': loop.epoch_num + loop.batch_num / loop.n_batches,\n",
    "            'batch_num' : loop.batch_num,\n",
    "            'accuracy': loop.accuracy.item(),\n",
    "            'loss': loop.loss.item(),\n",
    "            'lr': loop.optimizer.param_groups[0]['lr'],\n",
    "            'momentum': loop.optimizer.param_groups[0]['betas'][0],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopProgressBar(LoopComponent):\n",
    "\n",
    "    def __init__(self, item_names=['loss', 'accuracy']):\n",
    "        self.item_names = item_names\n",
    "\n",
    "    def on_epoch_begin(self, loop):\n",
    "        self.total, self.count = ({ name: 0.0 for name in self.item_names }, 0)\n",
    "        self.pbar = tqdm(total=loop.n_batches, desc=f\"{loop.epoch_desc} epoch {loop.epoch_num}\")\n",
    "\n",
    "    def on_batch_end(self, loop):\n",
    "        n = len(loop.batch)\n",
    "        self.count += n\n",
    "        for name in self.item_names:\n",
    "            self.total[name] += getattr(loop, name).item() * n\n",
    "        self.pbar.update(1)\n",
    "        if (not loop.is_training):\n",
    "            means = { f'mean_{name}': self.total[name] / self.count for name in self.item_names }\n",
    "            self.pbar.set_postfix(means)\n",
    "\n",
    "    def on_epoch_end(self, loop):\n",
    "        self.pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0fc5c9070c4efaad3edcb42148a576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=764, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095d65b47dec4e088a398a7aff878266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=3247202234, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of parameters: 777,577,733\n"
     ]
    }
   ],
   "source": [
    "# Seed RNG for replicability. Run at least a few times without seeding to measure performance.\n",
    "# torch.manual_seed(<type an int here>)\n",
    "\n",
    "# Make iterators for each split.\n",
    "trn_itr, val_itr, tst_itr = tt.data.Iterator.splits(\n",
    "    (trn_ds, val_ds, tst_ds),\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    device=DEVICE)\n",
    "\n",
    "# Initialize model.\n",
    "n_classes = len(_stoi)\n",
    "\n",
    "model = GPT2Classifier(n_classes)\n",
    "\n",
    "model = model.cuda(device=DEVICE)\n",
    "print('Total number of parameters: {:,}'.format(sum(np.prod(p.shape) for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 0: 100%|██████████| 2489/2489 [24:02<00:00,  1.82it/s]\n",
      "valid epoch 0: 100%|██████████| 18/18 [00:06<00:00,  2.25it/s, mean_loss=1.18, mean_accuracy=0.46] \n",
      "train epoch 1: 100%|██████████| 2489/2489 [24:07<00:00,  1.94it/s]\n",
      "valid epoch 1: 100%|██████████| 18/18 [00:06<00:00,  2.24it/s, mean_loss=1.1, mean_accuracy=0.524] \n",
      "train epoch 2: 100%|██████████| 2489/2489 [24:12<00:00,  1.95it/s]\n",
      "valid epoch 2: 100%|██████████| 18/18 [00:06<00:00,  2.24it/s, mean_loss=1.09, mean_accuracy=0.537]\n",
      "train epoch 3: 100%|██████████| 2489/2489 [24:11<00:00,  1.80it/s]\n",
      "valid epoch 3: 100%|██████████| 18/18 [00:06<00:00,  2.24it/s, mean_loss=1.09, mean_accuracy=0.555]\n",
      "train epoch 4: 100%|██████████| 2489/2489 [24:12<00:00,  1.56it/s]\n",
      "valid epoch 4: 100%|██████████| 18/18 [00:06<00:00,  2.25it/s, mean_loss=1.13, mean_accuracy=0.549]\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "loop = TrainTestLoop(model, [LoopMain(n_classes, DEVICE), LoopProgressBar()], trn_itr, val_itr)\n",
    "loop.train(n_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test epoch 5: 100%|██████████| 35/35 [00:12<00:00,  1.73it/s, mean_loss=1.07, mean_accuracy=0.538]\n"
     ]
    }
   ],
   "source": [
    "loop.test(tst_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
