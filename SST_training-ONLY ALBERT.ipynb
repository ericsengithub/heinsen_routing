{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of SST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext as tt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pytorch_extras import RAdam, SingleCycleScheduler\n",
    "from transformers import AlbertModel, AlbertTokenizer\n",
    "from pytorch_transformers import GPT2Model, GPT2Tokenizer\n",
    "from deps.torch_train_test_loop.torch_train_test_loop import LoopComponent, TrainTestLoop\n",
    "\n",
    "from models import SSTClassifier, GPT2Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained transformer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained GPT-2 loaded.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "#     'gpt2-large', do_lower_case=False)\n",
    "# lang_model = GPT2Model.from_pretrained(\n",
    "#     'gpt2-large', output_hidden_states=True, output_attentions=False)\n",
    "# lang_model.cuda(device=DEVICE)\n",
    "# lang_model.eval()\n",
    "print('Pretrained GPT-2 loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AlbertTokenizer.from_pretrained(\n",
    "    'albert-large-v2', do_lower_case=False)\n",
    "# lang_model = AlbertModel.from_pretrained(\n",
    "#     'albert-large-v2', output_hidden_states=True, output_attentions=False)\n",
    "# lang_model.cuda(device=DEVICE)\n",
    "# lang_model.eval()\n",
    "# print('Pretrained albert loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_texts_to_embs(tokenized_texts, pad_token=tokenizer.eos_token):\n",
    "    tokenized_texts = [[*tok_seq, pad_token] for tok_seq in tokenized_texts]\n",
    "    lengths = [len(tok_seq) for tok_seq in tokenized_texts]\n",
    "\n",
    "    max_length = max(lengths)\n",
    "    input_toks = [t + [pad_token] * (max_length - l) for t, l in zip(tokenized_texts, lengths)]\n",
    "\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(tok_seq) for tok_seq in input_toks]\n",
    "    input_ids = torch.tensor(input_ids).to(device=DEVICE)\n",
    "\n",
    "    mask = [[1.0] * length + [0.0] * (max_length - length) for length in lengths]\n",
    "    mask = torch.tensor(mask).to(device=DEVICE)  # [batch sz, num toks]\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs = lang_model(input_ids=input_ids)\n",
    "#         embs = torch.stack(outputs[-1], -2)  # [batch sz, n toks, n layers, d emb]\n",
    "\n",
    "    return mask, input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_grained = False  # set to False for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTFilter():\n",
    "\n",
    "    def __init__(self, remove_neutral=False, remove_dupes=False):\n",
    "        self.remove_neutral, self.remove_dupes  = (remove_neutral, remove_dupes)\n",
    "        self.prev_seen = {}\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        if self.remove_neutral and (sample.label == 'neutral'):\n",
    "            return False\n",
    "        hashable = ''.join(sample.text)\n",
    "        if self.remove_dupes and (hashable in self.prev_seen):\n",
    "            return False\n",
    "        self.prev_seen[hashable] = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets ready.\n",
      "Number of samples: 77,612 train phrases, 872 valid sentences, 1,821 test sentences.\n"
     ]
    }
   ],
   "source": [
    "# tt.datasets.SST.download(root='.data')  # download if necessary\n",
    "_stoi = { s: i for i, s in enumerate(\n",
    "    ['very negative', 'negative', 'neutral', 'positive', 'very positive'] \\\n",
    "    if fine_grained else ['negative', 'positive']\n",
    ") }\n",
    "TEXT = tt.data.RawField(\n",
    "    preprocessing=tokenizer.tokenize,\n",
    "    postprocessing=tokenized_texts_to_embs,\n",
    "    is_target=False)\n",
    "LABEL = tt.data.RawField(\n",
    "    postprocessing=lambda samples: torch.tensor([_stoi[s] for s in samples], device=DEVICE),\n",
    "    is_target=True)\n",
    "\n",
    "trn_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/train.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=True,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=True))\n",
    "val_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/dev.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    "tst_ds = tt.datasets.SST(\n",
    "    '.data/sst/trees/test.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "    filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    "\n",
    "print('Datasets ready.')\n",
    "print('Number of samples: {:,} train phrases, {:,} valid sentences, {:,} test sentences.'\\\n",
    "      .format(len(trn_ds), len(val_ds), len(tst_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _stoi = { s: i for i, s in enumerate(\n",
    "#     ['very negative', 'negative', 'neutral', 'positive', 'very positive'] \\\n",
    "#     if fine_grained else ['neg', 'pos']\n",
    "# ) }\n",
    "\n",
    "# TEXT = tt.data.RawField(\n",
    "#     preprocessing=tokenizer.tokenize,\n",
    "#     postprocessing=tokenized_texts_to_embs,\n",
    "#     is_target=False)\n",
    "# LABEL = tt.data.RawField(\n",
    "#     postprocessing=lambda samples: torch.tensor([_stoi[s] for s in samples], device=DEVICE),\n",
    "#     is_target=True)\n",
    "\n",
    "# trn_ds, tst_ds = tt.datasets.IMDB.splits(TEXT, LABEL)\n",
    "# # trn_ds = tt.datasets.IMDB(\n",
    "# #     '.data/sst/trees/train.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=True,\n",
    "# #     filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=True))\n",
    "# # tst_ds = tt.datasets.IMDB(\n",
    "# #     '.data/sst/trees/test.txt', TEXT, LABEL, fine_grained=fine_grained, subtrees=False,\n",
    "# #     filter_pred=SSTFilter(remove_neutral=(not fine_grained), remove_dupes=False))\n",
    " \n",
    "# trn_ds, val_ds = trn_ds.split()\n",
    "\n",
    "# print('Datasets ready.')\n",
    "# print('Number of samples: {:,} train phrases, {:,} valid sentences, {:,} test sentences.'\\\n",
    "#       .format(len(trn_ds), len(val_ds), len(tst_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.example.Example object at 0x7f2ff39a8780>\n"
     ]
    }
   ],
   "source": [
    "print(val_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopMain(LoopComponent):\n",
    "\n",
    "    def __init__(self, n_classes, device, pct_warmup=0.1, mixup=(0.2, 0.2)):\n",
    "        self.n_classes, self.device, self.pct_warmup = (n_classes, device, pct_warmup)\n",
    "        self.mixup_dist = torch.distributions.Beta(torch.tensor(mixup[0]), torch.tensor(mixup[1]))\n",
    "        self.onehot = torch.eye(self.n_classes, device=self.device)\n",
    "        self.saved_data = []\n",
    "\n",
    "    def on_train_begin(self, loop):\n",
    "        n_iters = len(loop.train_data) * loop.n_epochs\n",
    "        loop.optimizer = RAdam(loop.model.parameters(), lr=5e-4)\n",
    "        loop.scheduler = SingleCycleScheduler(\n",
    "            loop.optimizer, loop.n_optim_steps, frac=self.pct_warmup, min_lr=1e-5)\n",
    "        \n",
    "    def on_grads_reset(self, loop):\n",
    "        loop.model.zero_grad()\n",
    "\n",
    "    def on_forward_pass(self, loop):\n",
    "        model, batch = (loop.model, loop.batch)\n",
    "        mask, embs = batch.text\n",
    "        target_probs = self.onehot[batch.label]\n",
    "\n",
    "#         if loop.is_training:\n",
    "#             r = self.mixup_dist.sample([len(mask)]).to(device=mask.device)\n",
    "#             idx = torch.randperm(len(mask))\n",
    "#             mask = mask.lerp(mask[idx], r[:, None])\n",
    "#             embs = embs.lerp(embs[idx], r[:, None, None, None])\n",
    "#             target_probs = target_probs.lerp(target_probs[idx], r[:, None])\n",
    "\n",
    "        pred_scores = model(mask, embs)\n",
    "        _, pred_ids = pred_scores.max(-1)\n",
    "        accuracy = (pred_ids == batch.label).float().mean()\n",
    "\n",
    "        loop.pred_scores, loop.target_probs, loop.accuracy = (pred_scores, target_probs, accuracy)\n",
    "\n",
    "    def on_loss_compute(self, loop):\n",
    "        losses = -loop.target_probs * F.log_softmax(loop.pred_scores, dim=-1)  # CE\n",
    "        loop.loss = losses.sum(dim=-1).mean()  # sum of classes, mean of batch\n",
    "\n",
    "    def on_backward_pass(self, loop):\n",
    "        loop.loss.backward()\n",
    "\n",
    "    def on_optim_step(self, loop):\n",
    "        loop.optimizer.step()\n",
    "        loop.scheduler.step()\n",
    "\n",
    "    def on_batch_end(self, loop):\n",
    "        self.saved_data.append({\n",
    "            'n_samples': len(loop.batch),\n",
    "            'epoch_desc': loop.epoch_desc,\n",
    "            'epoch_num': loop.epoch_num,\n",
    "            'epoch_frac': loop.epoch_num + loop.batch_num / loop.n_batches,\n",
    "            'batch_num' : loop.batch_num,\n",
    "            'accuracy': loop.accuracy.item(),\n",
    "            'loss': loop.loss.item(),\n",
    "            'lr': loop.optimizer.param_groups[0]['lr'],\n",
    "            'momentum': loop.optimizer.param_groups[0]['betas'][0],\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoopProgressBar(LoopComponent):\n",
    "\n",
    "    def __init__(self, item_names=['loss', 'accuracy']):\n",
    "        self.item_names = item_names\n",
    "\n",
    "    def on_epoch_begin(self, loop):\n",
    "        self.total, self.count = ({ name: 0.0 for name in self.item_names }, 0)\n",
    "        self.pbar = tqdm(total=loop.n_batches, desc=f\"{loop.epoch_desc} epoch {loop.epoch_num}\")\n",
    "\n",
    "    def on_batch_end(self, loop):\n",
    "        n = len(loop.batch)\n",
    "        self.count += n\n",
    "        for name in self.item_names:\n",
    "            self.total[name] += getattr(loop, name).item() * n\n",
    "        self.pbar.update(1)\n",
    "        if (not loop.is_training):\n",
    "            means = { f'mean_{name}': self.total[name] / self.count for name in self.item_names }\n",
    "            self.pbar.set_postfix(means)\n",
    "\n",
    "    def on_epoch_end(self, loop):\n",
    "        self.pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 12,004,098\n",
      "<bound method Module.parameters of GPT2Classifier(\n",
      "  (GPT2): AlbertModel(\n",
      "    (embeddings): AlbertEmbeddings(\n",
      "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (encoder): AlbertTransformer(\n",
      "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
      "      (albert_layer_groups): ModuleList(\n",
      "        (0): AlbertLayerGroup(\n",
      "          (albert_layers): ModuleList(\n",
      "            (0): AlbertLayer(\n",
      "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): AlbertAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0, inplace=False)\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              )\n",
      "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (pooler_activation): Tanh()\n",
      "  )\n",
      "  (rnn): GRU(768, 64, batch_first=True, bidirectional=True)\n",
      "  (out): Linear(in_features=128, out_features=2, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Seed RNG for replicability. Run at least a few times without seeding to measure performance.\n",
    "torch.manual_seed(14)\n",
    "\n",
    "# Make iterators for each split.\n",
    "trn_itr, val_itr, tst_itr = tt.data.Iterator.splits(\n",
    "    (trn_ds, val_ds, tst_ds),\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    device=DEVICE)\n",
    "\n",
    "# Initialize model.\n",
    "n_classes = len(_stoi)\n",
    "\n",
    "model = GPT2Classifier(n_classes)\n",
    "\n",
    "model = model.cuda(device=DEVICE)\n",
    "print('Total number of parameters: {:,}'.format(sum(np.prod(p.shape) for p in model.parameters())))\n",
    "\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train epoch 0: 100%|██████████| 1213/1213 [02:34<00:00,  8.80it/s]\n",
      "valid epoch 0: 100%|██████████| 14/14 [00:01<00:00, 12.96it/s, mean_loss=0.43, mean_accuracy=0.783] \n",
      "train epoch 1: 100%|██████████| 1213/1213 [02:34<00:00,  7.84it/s]\n",
      "valid epoch 1: 100%|██████████| 14/14 [00:01<00:00, 13.13it/s, mean_loss=0.396, mean_accuracy=0.813]\n",
      "train epoch 2: 100%|██████████| 1213/1213 [02:33<00:00,  7.90it/s]\n",
      "valid epoch 2: 100%|██████████| 14/14 [00:01<00:00, 13.10it/s, mean_loss=0.397, mean_accuracy=0.819]\n",
      "test epoch 3: 100%|██████████| 29/29 [00:02<00:00, 13.57it/s, mean_loss=0.349, mean_accuracy=0.841]\n",
      "train epoch 0: 100%|██████████| 1213/1213 [02:33<00:00,  7.91it/s]\n",
      "valid epoch 0: 100%|██████████| 14/14 [00:01<00:00, 13.14it/s, mean_loss=0.4, mean_accuracy=0.826]  \n",
      "train epoch 1: 100%|██████████| 1213/1213 [02:34<00:00,  7.85it/s]\n",
      "valid epoch 1: 100%|██████████| 14/14 [00:01<00:00, 13.09it/s, mean_loss=0.487, mean_accuracy=0.817]\n",
      "train epoch 2:  46%|████▌     | 556/1213 [01:10<01:18,  8.41it/s]"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for i in range(3):\n",
    "    loop = TrainTestLoop(model, [LoopMain(n_classes, DEVICE), LoopProgressBar()], trn_itr, val_itr)\n",
    "    loop.train(n_epochs=3)\n",
    "    loop.test(tst_itr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop.test(tst_itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
